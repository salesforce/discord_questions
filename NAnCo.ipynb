{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use GPU: 2\n"
     ]
    }
   ],
   "source": [
    "import utils_misc\n",
    "utils_misc.select_freer_gpu()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from model_baselines import NLIAnswerEquivalence, STS_AE\n",
    "from model_consolidation import ConsolidationModel\n",
    "from utils_nanco import load_nanco_pairs\n",
    "import numpy as np, os, pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-18 23:03:37,706] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /export/home/models/quip-hf/ were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /export/home/models/quip-hf/ and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    # Sentence Similarity\n",
    "    {\"model_name\": \"STS-BertB\", \"model\": STS_AE(model_card=\"sentence-transformers/stsb-bert-base\")},\n",
    "    {\"model_name\": \"STS-MPNet\", \"model\": STS_AE(model_card=\"sentence-transformers/all-mpnet-base-v2\")},\n",
    "\n",
    "    # NLI\n",
    "    {\"model_name\": \"MNLI-Eonly\", \"model\": NLIAnswerEquivalence(model_card=\"roberta-large-mnli\", mode=\"eonly\")},\n",
    "    {\"model_name\": \"VitC-Eonly\", \"model\": NLIAnswerEquivalence(mode=\"eonly\")},\n",
    "\n",
    "    # Answer equivalence-based models\n",
    "    # {\"model_name\": \"lerc\", \"model\": LERCScorer(archive_path=\"/export/home/models/lerc-2020-11-18.tar.gz\")}, # Put a request on Github if you want this model\n",
    "    {\"model_name\": \"quip_reg_ae_mocha\", \"model\": ConsolidationModel(model_card=\"/export/home/models/quip-hf/\", model_file=\"/export/share/plaban/models/quip_ae_mocha_mae_0.5352.bin\")},\n",
    "    {\"model_name\": \"discord_qg/mocha\", \"model\": ConsolidationModel(model_card=\"Salesforce/qa_consolidation\")},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_threshold(scores, labels):\n",
    "    thresholds = np.arange(0, 5.0, 0.01)\n",
    "    max_acc = 0.0\n",
    "    best_threshold = 0.0\n",
    "    for t in thresholds:\n",
    "        y_pred = [1 if s > t else 0 for s in scores]\n",
    "        acc = balanced_accuracy_score(labels, y_pred)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_threshold = t\n",
    "    return best_threshold, max_acc\n",
    "\n",
    "def dataset_builder(ac_dataset):\n",
    "    questions = [d[\"question\"] for d in ac_dataset]\n",
    "    answers1 = [d[\"answer1\"] for d in ac_dataset]\n",
    "    answers2 = [d[\"answer2\"] for d in ac_dataset]\n",
    "    contexts1 = [d[\"paragraph1\"] for d in ac_dataset]\n",
    "    contexts2 = [d[\"paragraph2\"] for d in ac_dataset]\n",
    "    labels = [d[\"label\"] for d in ac_dataset]\n",
    "    return questions, answers1, answers2, contexts1, contexts2, labels\n",
    "\n",
    "def add_average_row(results):\n",
    "    average_row = {\"Question\": \"Average\"}\n",
    "    for mod in models:\n",
    "        average_row[mod[\"model_name\"]] = np.mean([r[mod[\"model_name\"]] for r in results])\n",
    "    results.append(average_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>STS-BertB</th>\n",
       "      <th>STS-MPNet</th>\n",
       "      <th>MNLI-Eonly</th>\n",
       "      <th>VitC-Eonly</th>\n",
       "      <th>quip_reg_ae_mocha</th>\n",
       "      <th>discord_qg/mocha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q2</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>0.783517</td>\n",
       "      <td>0.525317</td>\n",
       "      <td>0.541682</td>\n",
       "      <td>0.846658</td>\n",
       "      <td>0.904112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q3</td>\n",
       "      <td>0.986928</td>\n",
       "      <td>0.880719</td>\n",
       "      <td>0.790033</td>\n",
       "      <td>0.798611</td>\n",
       "      <td>0.996732</td>\n",
       "      <td>0.978350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q5</td>\n",
       "      <td>0.912811</td>\n",
       "      <td>0.797274</td>\n",
       "      <td>0.543675</td>\n",
       "      <td>0.615011</td>\n",
       "      <td>0.809285</td>\n",
       "      <td>0.879529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q6</td>\n",
       "      <td>0.973956</td>\n",
       "      <td>0.955495</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.590659</td>\n",
       "      <td>0.942088</td>\n",
       "      <td>0.955440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.946644</td>\n",
       "      <td>0.854251</td>\n",
       "      <td>0.607613</td>\n",
       "      <td>0.636491</td>\n",
       "      <td>0.898691</td>\n",
       "      <td>0.929358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question  STS-BertB  STS-MPNet  MNLI-Eonly  VitC-Eonly  quip_reg_ae_mocha  \\\n",
       "0       Q2   0.912879   0.783517    0.525317    0.541682           0.846658   \n",
       "1       Q3   0.986928   0.880719    0.790033    0.798611           0.996732   \n",
       "2       Q5   0.912811   0.797274    0.543675    0.615011           0.809285   \n",
       "3       Q6   0.973956   0.955495    0.571429    0.590659           0.942088   \n",
       "4  Average   0.946644   0.854251    0.607613    0.636491           0.898691   \n",
       "\n",
       "   discord_qg/mocha  \n",
       "0          0.904112  \n",
       "1          0.978350  \n",
       "2          0.879529  \n",
       "3          0.955440  \n",
       "4          0.929358  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "results = []\n",
    "model_thresholds = {mod[\"model_name\"]: [] for mod in models}\n",
    "for q in [\"Q2\", \"Q3\", \"Q5\", \"Q6\"]: # Validation questions\n",
    "    D = {\"Question\": q}\n",
    "    ac_dataset = load_nanco_pairs(q_includes=[q])\n",
    "    questions, answers1, answers2, contexts1, contexts2, labels = dataset_builder(ac_dataset)\n",
    "    \n",
    "    for mod in models:\n",
    "        val_scores = mod[\"model\"].score(questions=questions, answers1=answers1, answers2=answers2, contexts1=contexts1, contexts2=contexts2)[\"scores\"]\n",
    "        validation_threshold, best_val_acc = get_validation_threshold(val_scores, labels)\n",
    "        model_thresholds[mod[\"model_name\"]].append(validation_threshold)\n",
    "        D[mod[\"model_name\"]] = best_val_acc\n",
    "    results.append(D)\n",
    "add_average_row(results)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STS-BertB] Mean validation threshold: 0.488\n",
      "[STS-MPNet] Mean validation threshold: 0.463\n",
      "[MNLI-Eonly] Mean validation threshold: 0.385\n",
      "[VitC-Eonly] Mean validation threshold: 0.058\n",
      "[quip_reg_ae_mocha] Mean validation threshold: 1.330\n",
      "[discord_qg/mocha] Mean validation threshold: 2.745\n"
     ]
    }
   ],
   "source": [
    "# Threshold selection\n",
    "for model_name in model_thresholds:\n",
    "    print(\"[%s] Mean validation threshold: %.3f\" % (model_name, np.mean(model_thresholds[model_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>STS-BertB</th>\n",
       "      <th>STS-MPNet</th>\n",
       "      <th>MNLI-Eonly</th>\n",
       "      <th>VitC-Eonly</th>\n",
       "      <th>quip_reg_ae_mocha</th>\n",
       "      <th>discord_qg/mocha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q4</td>\n",
       "      <td>0.542101</td>\n",
       "      <td>0.706304</td>\n",
       "      <td>0.497609</td>\n",
       "      <td>0.512246</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.659130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q7</td>\n",
       "      <td>0.804275</td>\n",
       "      <td>0.775066</td>\n",
       "      <td>0.574490</td>\n",
       "      <td>0.578128</td>\n",
       "      <td>0.723267</td>\n",
       "      <td>0.888872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q8</td>\n",
       "      <td>0.607563</td>\n",
       "      <td>0.695238</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.752381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q9</td>\n",
       "      <td>0.978528</td>\n",
       "      <td>0.743152</td>\n",
       "      <td>0.748466</td>\n",
       "      <td>0.759860</td>\n",
       "      <td>0.939641</td>\n",
       "      <td>0.951475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.733117</td>\n",
       "      <td>0.729940</td>\n",
       "      <td>0.580141</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>0.740649</td>\n",
       "      <td>0.812965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Question  STS-BertB  STS-MPNet  MNLI-Eonly  VitC-Eonly  quip_reg_ae_mocha  \\\n",
       "0       Q4   0.542101   0.706304    0.497609    0.512246           0.594928   \n",
       "1       Q7   0.804275   0.775066    0.574490    0.578128           0.723267   \n",
       "2       Q8   0.607563   0.695238    0.500000    0.500000           0.704762   \n",
       "3       Q9   0.978528   0.743152    0.748466    0.759860           0.939641   \n",
       "4  Average   0.733117   0.729940    0.580141    0.587559           0.740649   \n",
       "\n",
       "   discord_qg/mocha  \n",
       "0          0.659130  \n",
       "1          0.888872  \n",
       "2          0.752381  \n",
       "3          0.951475  \n",
       "4          0.812965  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "results = []\n",
    "model_thresholds = {mn: np.mean(ts) for mn, ts in model_thresholds.items()}\n",
    "for q in [\"Q4\", \"Q7\", \"Q8\", \"Q9\"]: # Test questions\n",
    "    D = {\"Question\": q}\n",
    "    ac_dataset = load_nanco_pairs(q_includes=[q])\n",
    "    questions, answers1, answers2, contexts1, contexts2, labels = dataset_builder(ac_dataset)\n",
    "    \n",
    "    for mod in models:\n",
    "        test_scores = mod[\"model\"].score(questions=questions, answers1=answers1, answers2=answers2, contexts1=contexts1, contexts2=contexts2)[\"scores\"]\n",
    "        test_preds = [1 if s > model_thresholds[mod[\"model_name\"]] else 0 for s in test_scores]\n",
    "        test_bacc = balanced_accuracy_score(labels, test_preds)\n",
    "        D[mod[\"model_name\"]] = test_bacc\n",
    "    results.append(D)\n",
    "\n",
    "add_average_row(results)\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67c54bf5fb630355f694c741165b3bdb09e3b950d114a736855d00c72ee312e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
