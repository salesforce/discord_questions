{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use GPU: 1\n"
     ]
    }
   ],
   "source": [
    "import utils_misc\n",
    "utils_misc.select_freer_gpu()\n",
    "\n",
    "from utils_qg_eval import InqQG_Eval, load_discord_qg_eval_dataset\n",
    "from utils_qg_eval import compute_results\n",
    "import tqdm, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-14 10:56:17,777] [WARNING] [partition_parameters.py:53:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /export/home/models/quip-hf/ were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /export/home/models/quip-hf/ and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "iqg_eval = InqQG_Eval()\n",
    "dataset = load_discord_qg_eval_dataset()\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "Done loading models\n"
     ]
    }
   ],
   "source": [
    "from model_hf_generator import GeneratorHF\n",
    "\n",
    "qgens = [\n",
    "    # {\"model_name\": \"bartl_aqg_2ep\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"bartl_aqg_ep2_L_1.697.bin\")},\n",
    "    # {\"model_name\": \"bartl_aqg_4ep\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/share/plaban/models/discordqg/bartl_dqg_L_1.509.bin\")},\n",
    "    # {\"model_name\": \"bartl_1.43_fp16\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/home/models/bartl_dqg_bs32_L_1.433.bin\")},\n",
    "    # {\"model_name\": \"bartl_1.442_fp16\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/home/models/bartl_dqg_bs32_L_1.442.bin\")},\n",
    "    # {\"model_name\": \"bartl_1.456_fp16\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/home/models/bartl_dqg_bs32_L_1.456.bin\")},\n",
    "    {\"model_name\": \"bartl_1.483_fp16\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/home/models/bartl_dqg_bs32_L_1.483.bin\")},\n",
    "    {\"model_name\": \"bartl_1.504_fp16\", \"model\": GeneratorHF(model_card=\"facebook/bart-large\", starter_file=\"/export/home/models/bartl_dqg_bs32_L_1.504.bin\")},\n",
    "    # {\"model_name\": \"mixqgl_aqg\", \"model\": GeneratorHF(model_card=\"Salesforce/mixqg-large\", starter_file=\"mixqgl_aqg_L_1.164.bin\")},\n",
    "]\n",
    "\n",
    "for qgen in qgens:\n",
    "    if \"fp16\" in qgen[\"model_name\"]:\n",
    "        qgen[\"model\"].model.half()\n",
    "print(\"Done loading models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.41it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/export/home/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/export/home/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 100/100 [15:12<00:00,  9.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'QG Model': 'bartl_1.483_fp16', 'Score': 0.46, '%Analysis': 46.0, '%Factoid': 36.0, '%Unanswerable': 6.0, '%Vague': 12.0}, {'QG Model': 'bartl_1.504_fp16', 'Score': 0.39, '%Analysis': 39.0, '%Factoid': 45.0, '%Unanswerable': 9.0, '%Vague': 7.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "100%|██████████| 100/100 [14:32<00:00,  8.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'QG Model': 'bartl_1.483_fp16', 'Score': 0.69, '%Analysis': 69.0, '%Factoid': 16.0, '%Unanswerable': 11.0, '%Vague': 4.0}, {'QG Model': 'bartl_1.504_fp16', 'Score': 0.64, '%Analysis': 64.0, '%Factoid': 18.0, '%Unanswerable': 11.0, '%Vague': 7.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n",
      "100%|██████████| 100/100 [15:13<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'QG Model': 'bartl_1.483_fp16', 'Score': 0.58, '%Analysis': 58.0, '%Factoid': 37.0, '%Unanswerable': 2.0, '%Vague': 3.0}, {'QG Model': 'bartl_1.504_fp16', 'Score': 0.66, '%Analysis': 66.0, '%Factoid': 26.0, '%Unanswerable': 2.0, '%Vague': 6.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:08<00:00,  1.47it/s]\n",
      "100%|██████████| 100/100 [14:55<00:00,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'QG Model': 'bartl_1.483_fp16', 'Score': 0.13, '%Analysis': 13.0, '%Factoid': 75.0, '%Unanswerable': 6.0, '%Vague': 6.0}, {'QG Model': 'bartl_1.504_fp16', 'Score': 0.11, '%Analysis': 11.0, '%Factoid': 72.0, '%Unanswerable': 7.0, '%Vague': 10.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for start_word in [\"How\", \"Why\", \"What\", \"Who\"]:\n",
    "    gen_questions = {qgen[\"model_name\"]: [] for qgen in qgens}\n",
    "    # Generate the Questions\n",
    "    for d in tqdm.tqdm(dataset):\n",
    "        for qgen in qgens:\n",
    "            gen_qs = qgen[\"model\"].generate([d[\"summary\"]], max_gen_length=30, force_start=start_word)[0]\n",
    "            gen_questions[qgen[\"model_name\"]].append(gen_qs[0][\"output_text\"])\n",
    "\n",
    "    # Score the questions\n",
    "    iqg_scores = {qgen[\"model_name\"]: [] for qgen in qgens}\n",
    "    for i, context in enumerate(tqdm.tqdm(dataset)):\n",
    "        for qgen in qgens:\n",
    "            q = gen_questions[qgen[\"model_name\"]][i]\n",
    "            good_paragraphs = [p for a in context[\"articles\"] for p in a[\"paragraphs\"]]\n",
    "            other_paragraphs = context[\"other_paragraphs\"]\n",
    "            iqg_score = iqg_eval.score_one(q, good_paragraphs, other_paragraphs)\n",
    "            iqg_scores[qgen[\"model_name\"]].append(iqg_score)\n",
    "    print(compute_results(iqg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart_inqui:      43%, 65%, 42%, 13% -- 40.8%\n",
    "# t5l_dqg:         49%, 64%, 65%, 14% -- 48%\n",
    "# bartl_dqg_1.509: 34%, 73%  68%, 17% -- 48%\n",
    "# bartl_dqg_1.510: 35%, 68%, 56%, 08% -- 41.75\n",
    "# bartl_dqg_1.433: 41%, 69%, 63%, 15% -- 47.5%\n",
    "# bartl_dqg_1.442: 43%, 64%, 60%, 14% -- 45.25\n",
    "# bartl_dqg_1.456: 35%, 66%, 70%, 14% -- 46.25\n",
    "# bartl_dqg_1.483: 48%, 69%, 58%, 13% -- 47%\n",
    "# bartl_dqg_1.504: 39%, 64%, 66%, 11% -- 45%"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67c54bf5fb630355f694c741165b3bdb09e3b950d114a736855d00c72ee312e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
